{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'train.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "#I will have to remove the name field as this is irrelevant, the ticket field as this contains both letters and \n",
    "#numbers and the cabin field as the number of missing entries is high. Fares have been rounded.\n",
    "data.Fare = data.Fare.astype(int)\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "X = data[features]\n",
    "y = data.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train and test segments\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have resued my function to determine accuracy here, using the optimised settings from before\n",
    "def accuracy(train_X, valid_X, train_y, valid_y):\n",
    "    model = RandomForestRegressor(max_leaf_nodes = 17, random_state = 1)\n",
    "    model.fit(train_X, train_y)\n",
    "    predicted_values = model.predict(valid_X)\n",
    "\n",
    "    count = 0\n",
    "    for item in predicted_values:\n",
    "        item = round(item, 0)\n",
    "        predicted_values[count] = item\n",
    "        count+=1\n",
    "\n",
    "    real_values = []\n",
    "    for item in valid_y:\n",
    "        real_values.append(item)\n",
    "\n",
    "    count = 0\n",
    "    correct_predictions = 0\n",
    "    for item in predicted_values:\n",
    "        comparator = real_values[count]\n",
    "        if item == comparator:\n",
    "            correct_predictions+=1\n",
    "        count+=1\n",
    "    \n",
    "    accuracy = (correct_predictions/count)*100\n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we remove any categorical data - this will be sorted later and isn't missing many entries\n",
    "num_X_train = train_X.drop(['Sex', 'Embarked'], axis=1)\n",
    "num_X_valid = valid_X.drop(['Sex', 'Embarked'], axis=1)\n",
    "\n",
    "imputer = SimpleImputer(strategy = 'most_frequent')\n",
    "imputed_X_train = pd.DataFrame(imputer.fit_transform(num_X_train))\n",
    "imputed_X_valid = pd.DataFrame(imputer.transform(num_X_valid))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = num_X_train.columns\n",
    "imputed_X_valid.columns = num_X_valid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644 168 77\n",
      "577 314\n"
     ]
    }
   ],
   "source": [
    "# Get list of categorical variables\n",
    "s = (train_X.dtypes == 'object')\n",
    "object_columns = list(s[s].index)\n",
    "\n",
    "# Make copy to avoid changing original data (when imputing)\n",
    "train_X_plus = train_X[object_columns].copy()\n",
    "valid_X_plus = valid_X[object_columns].copy()\n",
    "\n",
    "#Here I am using for loops to imitiate the imputation for categorical data. This cannot be done using the imputer\n",
    "#itself as this only works for numerical data.\n",
    "S = 0\n",
    "C = 0\n",
    "Q = 0\n",
    "for item in train_X_plus['Embarked']:\n",
    "    if item == 'S':\n",
    "        S+=1\n",
    "    if item == 'C':\n",
    "        C+=1\n",
    "    if item == 'Q':\n",
    "        Q+=1\n",
    "for item in valid_X_plus['Embarked']:\n",
    "    if item == 'S':\n",
    "        S+=1\n",
    "    if item == 'C':\n",
    "        C+=1\n",
    "    if item == 'Q':\n",
    "        Q+=1\n",
    "print(S,C,Q)\n",
    "\n",
    "male = 0\n",
    "female = 0\n",
    "\n",
    "for item in train_X_plus['Sex']:\n",
    "    if item == 'male':\n",
    "        male+=1\n",
    "    if item == 'female':\n",
    "        female+=1\n",
    "for item in valid_X_plus['Sex']:\n",
    "    if item == 'male':\n",
    "        male+=1\n",
    "    if item == 'female':\n",
    "        female+=1\n",
    "print(male,female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in any gaps because we used the most frequent imputer earlier I will imitate the behaviour here\n",
    "\n",
    "train_X_plus['Embarked'] = train_X_plus['Embarked'].fillna(value = 'S', axis = 0)\n",
    "valid_X_plus['Embarked'] = valid_X_plus['Embarked'].fillna(value = 'S', axis = 0)\n",
    "train_X_plus['Sex'] = train_X_plus['Sex'].fillna(value = 'male', axis = 0)\n",
    "valid_X_plus['Sex'] = valid_X_plus['Sex'].fillna(value = 'male', axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will be able to use one hot encoding - similar to how before I manually used 1s and 0s to replace male and female values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_cols_train = pd.get_dummies(train_X_plus)\n",
    "OH_cols_valid = pd.get_dummies(valid_X_plus)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([imputed_X_train.reset_index(), OH_cols_train.reset_index()], axis=1)\n",
    "OH_X_valid = pd.concat([imputed_X_valid.reset_index(), OH_cols_valid.reset_index()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using modal imputation and one hot encoding =  81.61434977578476 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy using modal imputation and one hot encoding = ', accuracy(OH_X_train, OH_X_valid, train_y, valid_y), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model accuracy is lower than before, however now that I have been able to preprocess data, I have been able to use more of the available data. This will allow me to build my complex model - because the preprocessing is consistent and can be used for the official validation data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
